{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ihome/djishnu/alw399/.local/lib/python3.10/site-packages/louvain/__init__.py:54: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n",
      "/ix3/djishnu/alw399/envs/scGPT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.tasks import GeneEmbedding\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt.utils import set_seed\n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "n_hvg = 1200\n",
    "n_bins = 51\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = n_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume model from /ix3/djishnu/alw399/SpaceOracle/notebooks/benchmark/scGPT-human/best_model.pt, the model args will override the config /ix3/djishnu/alw399/SpaceOracle/notebooks/benchmark/scGPT-human/args.json.\n"
     ]
    }
   ],
   "source": [
    "# Specify model path; here we load the pre-trained scGPT blood model\n",
    "model_dir = Path(\"/ix3/djishnu/alw399/SpaceOracle/notebooks/benchmark/scGPT-human\") # downloaded pretrained model\n",
    "model_config_file = model_dir / \"args.json\"\n",
    "model_file = model_dir / \"best_model.pt\"\n",
    "vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "for s in special_tokens:\n",
    "    if s not in vocab:\n",
    "        vocab.append_token(s)\n",
    "\n",
    "# Retrieve model parameters from config files\n",
    "with open(model_config_file, \"r\") as f:\n",
    "    model_configs = json.load(f)\n",
    "print(\n",
    "    f\"Resume model from {model_file}, the model args will override the \"\n",
    "    f\"config {model_config_file}.\"\n",
    ")\n",
    "embsize = model_configs[\"embsize\"]\n",
    "nhead = model_configs[\"nheads\"]\n",
    "d_hid = model_configs[\"d_hid\"]\n",
    "nlayers = model_configs[\"nlayers\"]\n",
    "n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "\n",
    "gene2idx = vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "Loading params decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "Loading params decoder.fc.4.bias with shape torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): GeneEncoder(\n",
       "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
       "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_encoder): ContinuousValueEncoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.5, inplace=False)\n",
       "        (dropout2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ExprDecoder(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cls_decoder): ClsDecoder(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (sim): Similarity(\n",
       "    (cos): CosineSimilarity()\n",
       "  )\n",
       "  (creterion_cce): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    vocab=vocab,\n",
    "    pad_value=pad_value,\n",
    "    n_input_bins=n_input_bins,\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    print(f\"Loading all model params from {model_file}\")\n",
    "except:\n",
    "    # only load params that are in the model and match the size\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = torch.load(model_file)\n",
    "    pretrained_dict = {\n",
    "        k: v\n",
    "        for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    for k, v in pretrained_dict.items():\n",
    "        print(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad('/ix/djishnu/shared/djishnu_kor11/training_data_2025/snrna_human_tonsil.h5ad')\n",
    "\n",
    "adata.obs['batch'] = 'tonsil'\n",
    "\n",
    "ori_batch_col = \"batch\"\n",
    "adata.obs[\"celltype\"] = adata.obs[\"cell_type\"].astype(str)\n",
    "data_is_raw = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering genes by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Subsetting highly variable genes ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data following the scGPT data pre-processing pipeline\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=3,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=\"batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data-independent gene embeddings from scGPT\n",
    "gene_ids = np.array([id for id in gene2idx.values()])\n",
    "gene_embeddings = model.encoder(torch.tensor(gene_ids, dtype=torch.long).to(device))\n",
    "gene_embeddings = gene_embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved gene embeddings for 1192 genes.\n"
     ]
    }
   ],
   "source": [
    "# Filter on the intersection between the Immune Human HVGs found in step 1.2 and scGPT's 30+K foundation model vocab\n",
    "gene_embeddings = {gene: gene_embeddings[i] for i, gene in enumerate(gene2idx.keys()) if gene in adata.var.index.tolist()}\n",
    "print('Retrieved gene embeddings for {} genes.'.format(len(gene_embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1192/1192 [00:00<00:00, 2110430.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Construct gene embedding network\n",
    "embed = GeneEmbedding(gene_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 14:09:16.029308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-30 14:09:16.043516: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-30 14:09:16.047566: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-30 14:09:16.058336: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-30 14:09:17.607398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Perform Louvain clustering with desired resolution; here we specify resolution=40\n",
    "gdata = embed.get_adata(resolution=40)\n",
    "# Retrieve the gene clusters\n",
    "metagenes = embed.get_metagenes(gdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the set of gene programs from clusters with #genes >= 5\n",
    "mgs = dict()\n",
    "for mg, genes in metagenes.items():\n",
    "    if len(genes) > 4:\n",
    "        mgs[mg] = genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/ix/djishnu/shared/djishnu_kor11/scGPT_outputs/tonsil_mgs_pretrained.json', 'w') as f:\n",
    "    json.dump(mgs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 1192/1200 genes in vocabulary of size 60697.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding cells: 100%|██████████| 91/91 [00:03<00:00, 25.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 5778 × 1192\n",
       "    obs: 'cell_type', 'author_cell_type', 'cell_type_int', 'banksy_celltypes', 'cell_type_2', 'batch', 'celltype'\n",
       "    var: 'n_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection', 'index', 'id_in_vocab'\n",
       "    uns: 'cell_thresholds', 'cell_type_int_colors', 'received_ligands', 'received_ligands_tfl', 'hvg'\n",
       "    obsm: 'spatial', 'spatial_unscaled', 'bin_edges', 'X_scGPT'\n",
       "    layers: 'imputed_count', 'normalized_count', 'X_normed', 'X_binned'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_embed_adata = scg.tasks.embed_data(\n",
    "    adata,\n",
    "    model_dir,\n",
    "    gene_col='index',\n",
    "    batch_size=64,\n",
    "    return_new_adata=False,\n",
    ")\n",
    "\n",
    "ref_embed_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAACCCAAGCGCCTTG-1</th>\n",
       "      <td>0.012679</td>\n",
       "      <td>0.033990</td>\n",
       "      <td>-0.022575</td>\n",
       "      <td>-0.027113</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>-0.003039</td>\n",
       "      <td>0.016741</td>\n",
       "      <td>-0.010990</td>\n",
       "      <td>-0.005743</td>\n",
       "      <td>-0.010845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011440</td>\n",
       "      <td>-0.001572</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>-0.012034</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.038917</td>\n",
       "      <td>-0.012217</td>\n",
       "      <td>0.034038</td>\n",
       "      <td>-0.033374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCCAAGTGGACGT-1</th>\n",
       "      <td>0.055772</td>\n",
       "      <td>0.058613</td>\n",
       "      <td>-0.045575</td>\n",
       "      <td>-0.028732</td>\n",
       "      <td>0.023830</td>\n",
       "      <td>-0.038945</td>\n",
       "      <td>-0.011158</td>\n",
       "      <td>-0.005452</td>\n",
       "      <td>-0.005985</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020498</td>\n",
       "      <td>-0.018862</td>\n",
       "      <td>0.015582</td>\n",
       "      <td>0.025737</td>\n",
       "      <td>-0.004703</td>\n",
       "      <td>-0.014754</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>-0.041661</td>\n",
       "      <td>0.027332</td>\n",
       "      <td>-0.016341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCCACAGAAGTGC-1</th>\n",
       "      <td>0.042358</td>\n",
       "      <td>0.061969</td>\n",
       "      <td>-0.018097</td>\n",
       "      <td>-0.032435</td>\n",
       "      <td>0.015256</td>\n",
       "      <td>-0.023132</td>\n",
       "      <td>0.008818</td>\n",
       "      <td>-0.002340</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>-0.003514</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023124</td>\n",
       "      <td>-0.001365</td>\n",
       "      <td>0.032850</td>\n",
       "      <td>0.013485</td>\n",
       "      <td>-0.003391</td>\n",
       "      <td>-0.029026</td>\n",
       "      <td>0.008960</td>\n",
       "      <td>-0.010630</td>\n",
       "      <td>0.025934</td>\n",
       "      <td>-0.036061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCCAGTCATTGCA-1</th>\n",
       "      <td>0.026779</td>\n",
       "      <td>0.044952</td>\n",
       "      <td>-0.017846</td>\n",
       "      <td>-0.029361</td>\n",
       "      <td>0.026572</td>\n",
       "      <td>-0.001715</td>\n",
       "      <td>0.022163</td>\n",
       "      <td>-0.001436</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006393</td>\n",
       "      <td>-0.011647</td>\n",
       "      <td>0.007375</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>-0.013978</td>\n",
       "      <td>0.014779</td>\n",
       "      <td>0.023302</td>\n",
       "      <td>-0.016247</td>\n",
       "      <td>0.032590</td>\n",
       "      <td>-0.022023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCCATCATCGCAA-1</th>\n",
       "      <td>0.021346</td>\n",
       "      <td>0.043378</td>\n",
       "      <td>-0.035684</td>\n",
       "      <td>-0.012263</td>\n",
       "      <td>0.026250</td>\n",
       "      <td>-0.008220</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>-0.010840</td>\n",
       "      <td>-0.011226</td>\n",
       "      <td>-0.002133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006123</td>\n",
       "      <td>-0.026347</td>\n",
       "      <td>0.015204</td>\n",
       "      <td>0.021433</td>\n",
       "      <td>-0.020125</td>\n",
       "      <td>-0.009858</td>\n",
       "      <td>0.014942</td>\n",
       "      <td>-0.003959</td>\n",
       "      <td>0.033371</td>\n",
       "      <td>-0.034511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTTGCAGGGACTA-1</th>\n",
       "      <td>0.044359</td>\n",
       "      <td>0.042555</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>-0.015482</td>\n",
       "      <td>0.012198</td>\n",
       "      <td>-0.010583</td>\n",
       "      <td>0.005981</td>\n",
       "      <td>-0.009249</td>\n",
       "      <td>-0.020597</td>\n",
       "      <td>-0.016300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016108</td>\n",
       "      <td>-0.018804</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>0.024487</td>\n",
       "      <td>-0.003530</td>\n",
       "      <td>-0.012398</td>\n",
       "      <td>0.018969</td>\n",
       "      <td>-0.005771</td>\n",
       "      <td>0.036932</td>\n",
       "      <td>-0.020645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTTGCATTGTAGC-1</th>\n",
       "      <td>0.056101</td>\n",
       "      <td>0.047190</td>\n",
       "      <td>-0.047522</td>\n",
       "      <td>-0.031512</td>\n",
       "      <td>0.029193</td>\n",
       "      <td>-0.038083</td>\n",
       "      <td>-0.018171</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>-0.006285</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014117</td>\n",
       "      <td>-0.023777</td>\n",
       "      <td>0.004803</td>\n",
       "      <td>0.027973</td>\n",
       "      <td>0.010112</td>\n",
       "      <td>-0.005548</td>\n",
       "      <td>0.017982</td>\n",
       "      <td>-0.041940</td>\n",
       "      <td>0.016465</td>\n",
       "      <td>-0.026971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTTGGTACCACGC-1</th>\n",
       "      <td>0.040479</td>\n",
       "      <td>0.030753</td>\n",
       "      <td>-0.002240</td>\n",
       "      <td>-0.014950</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>-0.023222</td>\n",
       "      <td>-0.002127</td>\n",
       "      <td>-0.004745</td>\n",
       "      <td>-0.030631</td>\n",
       "      <td>-0.010744</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027701</td>\n",
       "      <td>-0.024816</td>\n",
       "      <td>0.011036</td>\n",
       "      <td>0.021804</td>\n",
       "      <td>-0.010900</td>\n",
       "      <td>-0.003421</td>\n",
       "      <td>0.009828</td>\n",
       "      <td>-0.026497</td>\n",
       "      <td>0.018989</td>\n",
       "      <td>-0.029901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTTGGTCTGTCCT-1</th>\n",
       "      <td>0.032625</td>\n",
       "      <td>0.033501</td>\n",
       "      <td>-0.036847</td>\n",
       "      <td>-0.035308</td>\n",
       "      <td>0.017868</td>\n",
       "      <td>-0.004681</td>\n",
       "      <td>0.010571</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>-0.006476</td>\n",
       "      <td>-0.014111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>-0.001332</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>-0.014333</td>\n",
       "      <td>-0.012018</td>\n",
       "      <td>0.046623</td>\n",
       "      <td>-0.009620</td>\n",
       "      <td>0.032957</td>\n",
       "      <td>-0.034998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTTGTCATGCGGC-1</th>\n",
       "      <td>0.018509</td>\n",
       "      <td>0.040287</td>\n",
       "      <td>-0.033941</td>\n",
       "      <td>-0.037706</td>\n",
       "      <td>0.017292</td>\n",
       "      <td>-0.017655</td>\n",
       "      <td>-0.003149</td>\n",
       "      <td>-0.006770</td>\n",
       "      <td>-0.003037</td>\n",
       "      <td>-0.008975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>-0.012566</td>\n",
       "      <td>0.028179</td>\n",
       "      <td>0.012970</td>\n",
       "      <td>-0.012459</td>\n",
       "      <td>-0.011980</td>\n",
       "      <td>0.029509</td>\n",
       "      <td>-0.013116</td>\n",
       "      <td>0.036020</td>\n",
       "      <td>-0.045433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5778 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0         1         2         3         4    \\\n",
       "NAME                                                                   \n",
       "AAACCCAAGCGCCTTG-1  0.012679  0.033990 -0.022575 -0.027113  0.002759   \n",
       "AAACCCAAGTGGACGT-1  0.055772  0.058613 -0.045575 -0.028732  0.023830   \n",
       "AAACCCACAGAAGTGC-1  0.042358  0.061969 -0.018097 -0.032435  0.015256   \n",
       "AAACCCAGTCATTGCA-1  0.026779  0.044952 -0.017846 -0.029361  0.026572   \n",
       "AAACCCATCATCGCAA-1  0.021346  0.043378 -0.035684 -0.012263  0.026250   \n",
       "...                      ...       ...       ...       ...       ...   \n",
       "TTTGTTGCAGGGACTA-1  0.044359  0.042555 -0.004852 -0.015482  0.012198   \n",
       "TTTGTTGCATTGTAGC-1  0.056101  0.047190 -0.047522 -0.031512  0.029193   \n",
       "TTTGTTGGTACCACGC-1  0.040479  0.030753 -0.002240 -0.014950  0.003016   \n",
       "TTTGTTGGTCTGTCCT-1  0.032625  0.033501 -0.036847 -0.035308  0.017868   \n",
       "TTTGTTGTCATGCGGC-1  0.018509  0.040287 -0.033941 -0.037706  0.017292   \n",
       "\n",
       "                         5         6         7         8         9    ...  \\\n",
       "NAME                                                                  ...   \n",
       "AAACCCAAGCGCCTTG-1 -0.003039  0.016741 -0.010990 -0.005743 -0.010845  ...   \n",
       "AAACCCAAGTGGACGT-1 -0.038945 -0.011158 -0.005452 -0.005985  0.002775  ...   \n",
       "AAACCCACAGAAGTGC-1 -0.023132  0.008818 -0.002340  0.000971 -0.003514  ...   \n",
       "AAACCCAGTCATTGCA-1 -0.001715  0.022163 -0.001436  0.007882  0.002111  ...   \n",
       "AAACCCATCATCGCAA-1 -0.008220  0.011903 -0.010840 -0.011226 -0.002133  ...   \n",
       "...                      ...       ...       ...       ...       ...  ...   \n",
       "TTTGTTGCAGGGACTA-1 -0.010583  0.005981 -0.009249 -0.020597 -0.016300  ...   \n",
       "TTTGTTGCATTGTAGC-1 -0.038083 -0.018171  0.002924 -0.006285  0.011071  ...   \n",
       "TTTGTTGGTACCACGC-1 -0.023222 -0.002127 -0.004745 -0.030631 -0.010744  ...   \n",
       "TTTGTTGGTCTGTCCT-1 -0.004681  0.010571  0.001721 -0.006476 -0.014111  ...   \n",
       "TTTGTTGTCATGCGGC-1 -0.017655 -0.003149 -0.006770 -0.003037 -0.008975  ...   \n",
       "\n",
       "                         502       503       504       505       506  \\\n",
       "NAME                                                                   \n",
       "AAACCCAAGCGCCTTG-1 -0.011440 -0.001572  0.000572  0.003772 -0.012034   \n",
       "AAACCCAAGTGGACGT-1 -0.020498 -0.018862  0.015582  0.025737 -0.004703   \n",
       "AAACCCACAGAAGTGC-1 -0.023124 -0.001365  0.032850  0.013485 -0.003391   \n",
       "AAACCCAGTCATTGCA-1 -0.006393 -0.011647  0.007375  0.000434 -0.013978   \n",
       "AAACCCATCATCGCAA-1 -0.006123 -0.026347  0.015204  0.021433 -0.020125   \n",
       "...                      ...       ...       ...       ...       ...   \n",
       "TTTGTTGCAGGGACTA-1 -0.016108 -0.018804  0.023457  0.024487 -0.003530   \n",
       "TTTGTTGCATTGTAGC-1 -0.014117 -0.023777  0.004803  0.027973  0.010112   \n",
       "TTTGTTGGTACCACGC-1 -0.027701 -0.024816  0.011036  0.021804 -0.010900   \n",
       "TTTGTTGGTCTGTCCT-1  0.004110 -0.001332  0.010089  0.001788 -0.014333   \n",
       "TTTGTTGTCATGCGGC-1  0.006444 -0.012566  0.028179  0.012970 -0.012459   \n",
       "\n",
       "                         507       508       509       510       511  \n",
       "NAME                                                                  \n",
       "AAACCCAAGCGCCTTG-1  0.001211  0.038917 -0.012217  0.034038 -0.033374  \n",
       "AAACCCAAGTGGACGT-1 -0.014754  0.002766 -0.041661  0.027332 -0.016341  \n",
       "AAACCCACAGAAGTGC-1 -0.029026  0.008960 -0.010630  0.025934 -0.036061  \n",
       "AAACCCAGTCATTGCA-1  0.014779  0.023302 -0.016247  0.032590 -0.022023  \n",
       "AAACCCATCATCGCAA-1 -0.009858  0.014942 -0.003959  0.033371 -0.034511  \n",
       "...                      ...       ...       ...       ...       ...  \n",
       "TTTGTTGCAGGGACTA-1 -0.012398  0.018969 -0.005771  0.036932 -0.020645  \n",
       "TTTGTTGCATTGTAGC-1 -0.005548  0.017982 -0.041940  0.016465 -0.026971  \n",
       "TTTGTTGGTACCACGC-1 -0.003421  0.009828 -0.026497  0.018989 -0.029901  \n",
       "TTTGTTGGTCTGTCCT-1 -0.012018  0.046623 -0.009620  0.032957 -0.034998  \n",
       "TTTGTTGTCATGCGGC-1 -0.011980  0.029509 -0.013116  0.036020 -0.045433  \n",
       "\n",
       "[5778 rows x 512 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df = pd.DataFrame(ref_embed_adata.obsm['X_scGPT'], index=ref_embed_adata.obs.index)\n",
    "embed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df.to_parquet('/ix/djishnu/shared/djishnu_kor11/scGPT_outputs/tonsil_embeddings_pretrained.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f38a8427daffacdadbb8e2da33b6675ddd8f99c92370a357f616d8c5d64c35dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
